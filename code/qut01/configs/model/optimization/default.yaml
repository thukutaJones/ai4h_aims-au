# For more info on the contents and default values in this file, refer to Lightning:
# https://lightning.ai/docs/pytorch/stable/common/lightning_module.html#configure-optimizers
# (parameters and defaults last checked on 2023/10/04 for Lightning 2.0.1)

optimizer: # note: the optimizer will always be created while providing it with model parameters
  _target_: torch.optim.Adam # it's a good default, really! https://arxiv.org/abs/1910.05446
  # lr: 0.001
  # betas: [0.9, 0.999]
  # eps: 1e-08
  # weight_decay: 0
  # amsgrad: False

#lr_scheduler: # note: the scheduler will always be created while providing it with the optimizer
#  scheduler: # this field is actually MANDATORY when specifying the lr scheduler config!
#    _target_: torch.optim.lr_scheduler.StepLRR # just a dummy one, really, pick whatever you want
#    #step_size: 1000 # again, unlikely we get there, this is just a dummy value
#  interval: epoch # default unit of the scheduler's step size (could also be 'step')
#  frequency: 1 # default number of epochs/steps that should pass between `scheduler.step()` calls
#  monitor: ${target_metric} # name of the metric to monitor (if needed, for some scheduler types)
#  strict: True # toggles whether the monitor metric must be strictly found or not when needed
#  name: null # related to the `LearningRateMonitor` callback for monitoring (likely not needed)

freeze_no_grad_params: True # toggles whether to freeze (wrt regularization/momentum) no-grad params
