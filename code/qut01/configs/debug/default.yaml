# @package _global_
# This config will be loaded in all debug sessions in order to simplify the debugging process!
# (we'll run single-epoch training sessions with deterministic behavior and verbose settings)

#callbacks:
#  gradient_check:
#    # disabled as of 2023-03-15 since it comes from pl_bolts which is just about ALWAYS BROKEN
#    # https://lightning-bolts.readthedocs.io/en/latest/callbacks/monitor.html
#    _target_: pl_bolts.callbacks.BatchGradientVerificationCallback
#  save_ckpt_on_exception:
#    # https://lightning.ai/docs/pytorch/stable/api/lightning.pytorch.callbacks.OnExceptionCheckpoint.html
#    _target_: lightning.pytorch.callbacks.OnExceptionCheckpoint

data:
  datamodule:
    dataloader_configs:
      _default_: # this group provides shared (but overridable) settings for all data loader types
        num_workers: 0 # debuggers don't like multiprocessing, let's run everything on the main thread
        pin_memory: False # disable gpu memory pin (no pre-allocation on the GPU at all might help)
        shuffle: False
        persistent_workers: False
      train:
        num_workers: 0
        pin_memory: False
        shuffle: False
        persistent_workers: False
      valid:
        num_workers: 0
        pin_memory: False
        shuffle: False
        persistent_workers: False
      test:
        num_workers: 0
        pin_memory: False
        shuffle: False
        persistent_workers: False
      predict:
        num_workers: 0
        pin_memory: False
        shuffle: False
        persistent_workers: False

hydra:
  verbose: True # this will set the level of all command line loggers to 'DEBUG' for max verbosity

trainer:
  accelerator: cpu # run on CPU only (might be really slow, but change it if necessary)
  strategy: auto # fallback to whatever is simple and supported on basic accelerators
  devices: 1 # debuggers don't like multi-device setups, let's not use anything fancy here
  num_nodes: 1 # debuggers don't like multi-device setups, let's not use anything fancy here
  precision: 32-true # fallback to the default value that we know won't be funky
  max_epochs: 1 # run a single epoch by default (other debug configs might try overfit instead)
  limit_train_batches: 5 # run at most 5 batches per training epoch
  limit_val_batches: 5 # run at most 5 batches per validation epoch
  limit_test_batches: 5 # run at most 5 batches per testing epoch
  limit_predict_batches: 5 # run at most 5 batches per predict epoch
  num_sanity_val_steps: 2 # run through two validation batches before training
  accumulate_grad_batches: 1 # fallback to the simplest gradient accumulation case
  # might want to add gradient_clip_val/gradient_clip_algorithm here?
  deterministic: True # we'd like each session to behave the same as the last
  benchmark: False # turn benchmark mode off, we don't need crazy performance here
  inference_mode: True # not sure why this should ever be off, but let's make sure it's not
  detect_anomaly: True # raise an exception if NaN or +/-inf is detected in any tensor

utils:
  output_dir_prefix: ${utils.output_root_dir}/debug
  print_config: True # toggles whether to pretty-print the config at the start of the run
  ignore_warnings: False # let's always enable all warnings by default in the debug configs
  log_installed_pkgs: True # always log all installed packages in the python env to the output dir
  log_runtime_tags: True # always log all runtime tags (e.g. platform name) to the output dir
  log_interpolated_config: True # always log the hydra config with interpolated params
  seed: 1337 # might help make sure the RNG behavior is static
  seed_workers: True # also fix that seed across all data loader workers
